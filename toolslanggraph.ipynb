{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tool-Augmented LangGraph RAG\n",
        "\n",
        "This notebook extends the base LangGraph pipeline with **tool routing**. A controller decides whether to answer using our uploaded PDFs or to call an external search tool (Tavily) when internal knowledge is insufficient. Docstrings describe when each tool should be chosen so the language model understands *why/when/how* to invoke them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Asus\\Desktop\\all projects & learnings\\llm-rag-fastapi-study\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os  # Filesystem helper to list resume PDFs.\n",
        "from typing import TypedDict  # Keeps LangGraph state strongly typed.\n",
        "\n",
        "from IPython.display import Image, display  # Used later to render the graph PNG inline.\n",
        "from langchain_community.document_loaders import PyPDFLoader  # Same loader stack as other notebooks.\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter  # Creates overlap-aware chunks.\n",
        "from langchain_huggingface import HuggingFaceEmbeddings  # Provides MiniLM embeddings.\n",
        "from langchain_community.vectorstores import FAISS  # In-memory FAISS index.\n",
        "from langchain_ollama import OllamaLLM  # Local LLM that powers both router + answer nodes.\n",
        "from langchain_core.prompts import ChatPromptTemplate  # Prompt templating utility for router + answer.\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults  # Web-search tool (requires TAVILY_API_KEY).\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END  # LangGraph primitives.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Embed Internal Documents\n",
        "We keep the ingestion identical to the previous notebooks so the only new behavior comes from the tool-routing logic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DOCS_FOLDER = r\"C:\\\\Users\\\\Asus\\\\Documents\\\\all_doc\"  # Reuse the same resume/document directory.\n",
        "\n",
        "documents = []  # Each entry becomes a LangChain Document created per PDF page.\n",
        "for file in os.listdir(DOCS_FOLDER):  # Loop across directory contents.\n",
        "    if file.lower().endswith(\".pdf\"):  # Only parse PDFs.\n",
        "        loader = PyPDFLoader(os.path.join(DOCS_FOLDER, file))  # Keeps metadata like source filename + page.\n",
        "        documents.extend(loader.load())  # Append every page Document into the corpus list.\n",
        "\n",
        "len(documents)  # Gives visibility into how many pages will move through chunking.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,  # Maintains parity with other notebooks.\n",
        "    chunk_overlap=10,  # Protects sentence continuity.\n",
        ")\n",
        "chunks = splitter.split_documents(documents)  # Expand into chunked Document objects.\n",
        "print(f\"Split into {len(chunks)} chunks\")\n",
        "\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")  # MiniLM embeddings.\n",
        "vector_store = FAISS.from_documents(chunks, embedding)  # Store embeddings in FAISS for similarity search.\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})  # Search top-10 by cosine similarity.\n",
        "print(f\"Vector store holds {vector_store.index.ntotal} rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define Tooling Layer\n",
        "We expose two tools:\n",
        "\n",
        "1. **InternalDocsTool** – pulls context from the FAISS retriever. Docstring explains it should be used for resume-specific questions.\n",
        "2. **TavilySearchTool** – hits the public web (needs `TAVILY_API_KEY`). Docstring tells the LLM to call it when users ask for fresh/external info.\n",
        "\n",
        "LangChain/agents rely heavily on docstrings/descriptions to decide which tool to call, so we surface the same concept here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()  # loads .env file automatically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tavily = TavilySearchResults(\n",
        "    max_results=3  # Limit API usage + keep responses concise.\n",
        ")  # Requires `TAVILY_API_KEY` to be set in the environment before execution.\n",
        "\n",
        "\n",
        "def internal_docs_tool(question: str) -> str:\n",
        "    \"\"\"Use this tool when the answer likely exists inside the uploaded resumes or project PDFs.\"\"\"\n",
        "\n",
        "    docs = retriever.invoke(question)  # Hit FAISS for semantic matches.\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)  # Return concatenated passages.\n",
        "\n",
        "\n",
        "def tavily_search_tool(question: str) -> str:\n",
        "    \"\"\"Use this tool for real-time/company-wide facts that are NOT covered by the uploads.\"\"\"\n",
        "\n",
        "    results = tavily.invoke({\"query\": question})  # Returns JSON-friendly search summaries.\n",
        "    return \"\\n\".join(hit[\"content\"] for hit in results[\"results\"])  # Collapse into plain text for the LLM.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. LangGraph State + Router Prompt\n",
        "The router LLM reads both docstrings so it understands *why* each tool exists. It outputs one of `internal_docs`, `web_search`, or `hybrid` (use both).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ToolRAGState(TypedDict, total=False):\n",
        "    \"\"\"Shared state for the tool-enabled graph.\"\"\"\n",
        "\n",
        "    question: str  # Original user query.\n",
        "    selected_tool: str  # Router decision: internal_docs / web_search / hybrid.\n",
        "    internal_context: str  # Text retrieved from FAISS.\n",
        "    web_context: str  # Text returned by Tavily.\n",
        "    answer: str  # Final LLM response.\n",
        "\n",
        "TOOL_DOCS = {\n",
        "    \"internal_docs\": internal_docs_tool.__doc__,\n",
        "    \"web_search\": tavily_search_tool.__doc__,\n",
        "}\n",
        "\n",
        "tool_routing_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are a routing model that decides which tool to call before answering a question.\n",
        "Available tools:\n",
        "\n",
        "- internal_docs: {internal_docs_doc}\n",
        "- web_search: {web_search_doc}\n",
        "\n",
        "Return one of: internal_docs, web_search, hybrid.\n",
        "Question: {question}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "router_llm = OllamaLLM(model=\"llama3.2\")  # Reuse the same local model for routing decisions.\n",
        "answer_llm = OllamaLLM(model=\"llama3.2\")  # Separate instance for clarity (could be shared).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def router_node(state: ToolRAGState) -> ToolRAGState:\n",
        "    \"\"\"Ask the LLM which tool(s) should run given the question + docstrings.\"\"\"\n",
        "\n",
        "    decision = router_llm.invoke(\n",
        "        tool_routing_prompt.format(\n",
        "            question=state[\"question\"],\n",
        "            internal_docs_doc=TOOL_DOCS[\"internal_docs\"],\n",
        "            web_search_doc=TOOL_DOCS[\"web_search\"],\n",
        "        )\n",
        "    ).strip().lower()\n",
        "    normalized = decision if decision in {\"internal_docs\", \"web_search\", \"hybrid\"} else \"internal_docs\"\n",
        "    return {**state, \"selected_tool\": normalized}\n",
        "\n",
        "\n",
        "def internal_retrieve_node(state: ToolRAGState) -> ToolRAGState:\n",
        "    \"\"\"Populate `internal_context` when the router requested internal knowledge.\"\"\"\n",
        "\n",
        "    context = internal_docs_tool(state[\"question\"])\n",
        "    return {**state, \"internal_context\": context}\n",
        "\n",
        "\n",
        "def web_search_node(state: ToolRAGState) -> ToolRAGState:\n",
        "    \"\"\"Populate `web_context` via Tavily when the router asked for external info.\"\"\"\n",
        "\n",
        "    context = tavily_search_tool(state[\"question\"])\n",
        "    return {**state, \"web_context\": context}\n",
        "\n",
        "\n",
        "def answer_node(state: ToolRAGState) -> ToolRAGState:\n",
        "    \"\"\"Combine whichever contexts exist and ask the answering LLM to respond.\"\"\"\n",
        "\n",
        "    context_sections = []\n",
        "    if \"internal_context\" in state:\n",
        "        context_sections.append(\"INTERNAL DOCS\\n\" + state[\"internal_context\"])\n",
        "    if \"web_context\" in state:\n",
        "        context_sections.append(\"WEB SEARCH\\n\" + state[\"web_context\"])\n",
        "\n",
        "    prompt = f\"\"\"Use the consolidated evidence below to answer the question.\n",
        "{''.join(section + '\\n\\n' for section in context_sections)}\n",
        "Question: {state['question']}\n",
        "\"\"\"\n",
        "    answer = answer_llm.invoke(prompt)\n",
        "    return {**state, \"answer\": answer}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Wire the LangGraph with Conditional Edges\n",
        "Conditional edges let us branch based on the router decision. Hybrid mode simply runs internal retrieval first, then falls through to web search before synthesis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def route_from_router(state: ToolRAGState) -> str:\n",
        "    \"\"\"Return the next node label right after the router.\"\"\"\n",
        "\n",
        "    return state[\"selected_tool\"]\n",
        "\n",
        "\n",
        "def route_after_internal(state: ToolRAGState) -> str:\n",
        "    \"\"\"Decide whether to run web search next (hybrid) or go straight to answering.\"\"\"\n",
        "\n",
        "    return \"web_search\" if state[\"selected_tool\"] == \"hybrid\" else \"answer\"\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(ToolRAGState)\n",
        "\n",
        "graph_builder.add_node(\"router\", router_node)\n",
        "graph_builder.add_node(\"internal\", internal_retrieve_node)\n",
        "graph_builder.add_node(\"web\", web_search_node)\n",
        "graph_builder.add_node(\"answer\", answer_node)\n",
        "\n",
        "graph_builder.add_edge(START, \"router\")\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"router\",\n",
        "    route_from_router,\n",
        "    {\n",
        "        \"internal_docs\": \"internal\",\n",
        "        \"web_search\": \"web\",\n",
        "        \"hybrid\": \"internal\",\n",
        "    },\n",
        ")\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"internal\",\n",
        "    route_after_internal,\n",
        "    {\n",
        "        \"web_search\": \"web\",\n",
        "        \"answer\": \"answer\",\n",
        "    },\n",
        ")\n",
        "\n",
        "graph_builder.add_edge(\"web\", \"answer\")\n",
        "graph_builder.add_edge(\"answer\", END)\n",
        "\n",
        "tools_graph = graph_builder.compile()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Run Sample Questions\n",
        "Try one question that internal docs can answer and another that requires web search to demonstrate the router + docstrings at work.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"list the projects done in meta and teksystems\",  # Should route to internal_docs.\n",
        "    \"latest news about teksystems partnerships\",  # Likely requires web_search.\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    result = tools_graph.invoke({\"question\": question})\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Selected tool: {result.get('selected_tool')}\")\n",
        "    print(f\"Answer: {result.get('answer')}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualize Tool Routing\n",
        "As before, we export Mermaid text + PNG so the control flow is easy to inspect.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mermaid_text = tools_graph.get_graph().draw_mermaid()  # Text version for docs.\n",
        "print(mermaid_text)\n",
        "\n",
        "with open(\"toolslanggraph.mmd\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(mermaid_text)\n",
        "\n",
        "mermaid_png = tools_graph.get_graph().draw_mermaid_png()\n",
        "with open(\"toolslanggraph.png\", \"wb\") as f:\n",
        "    f.write(mermaid_png)\n",
        "\n",
        "display(Image(mermaid_png))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Takeaways\n",
        "- Docstrings feed the router prompt, so the LLM knows *when* to call each tool.\n",
        "- Conditional LangGraph edges make it easy to extend the flow (add evaluators, safety checks, etc.).\n",
        "- Both the internal FAISS retriever and Tavily outputs remain visible in the final state, enabling detailed telemetry or UI surfacing.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
